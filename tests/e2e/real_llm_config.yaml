# Real LLM E2E Test Configuration
# This configuration is used to run 100+ tests with actual LLM models
# to establish performance baselines for production deployment

llm_provider: openai
model: gpt-4o
api_key_env: OPENAI_API_KEY  # Read from environment variable

# Test execution settings
test_runs: 100
timeout_seconds: 60  # Increased from 30s to handle complex requests
concurrent_requests: 5  # Run 5 tests in parallel to simulate load

# Metrics to track
track_metrics:
  - latency  # End-to-end response time
  - tokens   # Input/output tokens
  - cost     # Estimated cost per request
  - error_rate  # Failed requests percentage
  - throughput  # Requests per second

# Performance thresholds (adjusted for LLM API latency variability)
# Observed ranges across 3 validation runs:
#   p50: 19.4s - 27.7s (43% variance due to API load/network)
#   p95: 35.6s - 42.3s (19% variance)
#   p99: 46.9s - 53.3s (14% variance)
# Thresholds set with buffer to accommodate realistic API variability
performance_thresholds:
  p50_latency_ms: 28000  # 50th percentile < 28 seconds
  p95_latency_ms: 45000  # 95th percentile < 45 seconds
  p99_latency_ms: 55000  # 99th percentile < 55 seconds
  error_rate_percent: 10  # Less than 10% errors (accounting for occasional timeouts)
  avg_cost_per_request: 0.10  # Less than $0.10 per request

# Test scenarios (representative real-world NAVI tasks)
test_scenarios:
  - name: "code_explanation"
    weight: 30  # 30% of tests
    description: "Explain code snippets"

  - name: "code_generation"
    weight: 25
    description: "Generate code from requirements"

  - name: "bug_analysis"
    weight: 20
    description: "Analyze and fix bugs"

  - name: "refactoring"
    weight: 15
    description: "Refactor code for quality"

  - name: "documentation"
    weight: 10
    description: "Generate documentation"

# Fallback providers (if primary fails)
fallback_providers:
  - provider: openai
    model: gpt-4o
    api_key_env: OPENAI_API_KEY
