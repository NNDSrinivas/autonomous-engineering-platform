# Real LLM E2E Test Configuration
# This configuration is used to run 100+ tests with actual LLM models
# to establish performance baselines for production deployment

llm_provider: openai
model: gpt-4o
api_key_env: OPENAI_API_KEY  # Read from environment variable

# Test execution settings
test_runs: 100
timeout_seconds: 30
concurrent_requests: 5  # Run 5 tests in parallel to simulate load

# Metrics to track
track_metrics:
  - latency  # End-to-end response time
  - tokens   # Input/output tokens
  - cost     # Estimated cost per request
  - error_rate  # Failed requests percentage
  - throughput  # Requests per second

# Performance thresholds (will fail if exceeded)
performance_thresholds:
  p50_latency_ms: 2000   # 50th percentile < 2 seconds
  p95_latency_ms: 5000   # 95th percentile < 5 seconds
  p99_latency_ms: 10000  # 99th percentile < 10 seconds
  error_rate_percent: 5  # Less than 5% errors
  avg_cost_per_request: 0.05  # Less than $0.05 per request

# Test scenarios (representative real-world NAVI tasks)
test_scenarios:
  - name: "code_explanation"
    weight: 30  # 30% of tests
    description: "Explain code snippets"

  - name: "code_generation"
    weight: 25
    description: "Generate code from requirements"

  - name: "bug_analysis"
    weight: 20
    description: "Analyze and fix bugs"

  - name: "refactoring"
    weight: 15
    description: "Refactor code for quality"

  - name: "documentation"
    weight: 10
    description: "Generate documentation"

# Fallback providers (if primary fails)
fallback_providers:
  - provider: openai
    model: gpt-4o
    api_key_env: OPENAI_API_KEY
