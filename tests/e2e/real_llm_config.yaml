# Real LLM E2E Test Configuration
# This configuration is used to run 100+ tests with actual LLM models
# to establish performance baselines for production deployment

llm_provider: openai
model: gpt-4o
api_key_env: OPENAI_API_KEY  # Read from environment variable

# Test execution settings
test_runs: 100
timeout_seconds: 60  # Increased from 30s to handle complex requests
concurrent_requests: 5  # Run 5 tests in parallel to simulate load

# Metrics to track
track_metrics:
  - latency  # End-to-end response time
  - tokens   # Input/output tokens
  - cost     # Estimated cost per request
  - error_rate  # Failed requests percentage
  - throughput  # Requests per second

# Performance thresholds (adjusted to realistic LLM API latency)
# Based on actual measurements: avg=13s, min=1.7s, max=29s
performance_thresholds:
  p50_latency_ms: 15000  # 50th percentile < 15 seconds (realistic for LLM calls)
  p95_latency_ms: 25000  # 95th percentile < 25 seconds
  p99_latency_ms: 35000  # 99th percentile < 35 seconds
  error_rate_percent: 10  # Less than 10% errors (accounting for occasional timeouts)
  avg_cost_per_request: 0.10  # Less than $0.10 per request

# Test scenarios (representative real-world NAVI tasks)
test_scenarios:
  - name: "code_explanation"
    weight: 30  # 30% of tests
    description: "Explain code snippets"

  - name: "code_generation"
    weight: 25
    description: "Generate code from requirements"

  - name: "bug_analysis"
    weight: 20
    description: "Analyze and fix bugs"

  - name: "refactoring"
    weight: 15
    description: "Refactor code for quality"

  - name: "documentation"
    weight: 10
    description: "Generate documentation"

# Fallback providers (if primary fails)
fallback_providers:
  - provider: openai
    model: gpt-4o
    api_key_env: OPENAI_API_KEY
